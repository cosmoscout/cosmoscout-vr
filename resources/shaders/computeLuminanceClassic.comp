////////////////////////////////////////////////////////////////////////////////////////////////////
//                               This file is part of CosmoScout VR                               //
////////////////////////////////////////////////////////////////////////////////////////////////////

// SPDX-FileCopyrightText: German Aerospace Center (DLR) <cosmoscout@dlr.de>
// SPDX-License-Identifier: MIT

#if NUM_MULTISAMPLES > 0
    layout (rgba32f, binding = 0) readonly uniform image2DMS uInHDRBuffer;
#else
    layout (rgba32f, binding = 0) readonly uniform image2D uInHDRBuffer;
#endif

layout (rg32f, binding = 1) uniform image1D uOutLuminance;

// Returns the luminance for the pixel.
float sampleHDRBuffer(ivec2 pos) {
    #if NUM_MULTISAMPLES > 0
  vec3 color = vec3(0.0);
    for (int i = 0; i < NUM_MULTISAMPLES; ++i) {
        color += imageLoad(uInHDRBuffer, pos, i).rgb;
    }
    color /= NUM_MULTISAMPLES;
    #else
  vec3 color = imageLoad(uInHDRBuffer, pos).rgb;
    #endif
return max(max(color.r, color.g), color.b);
}

ivec2 indexToPos(uint index, int width) {
    return ivec2(index % width, index / width);
}


layout (local_size_x = 1024) in;

shared float sSum[1024];
shared float sMax[1024];

// This shader does a standard parallel reduction based on a CUDA webinar:
// https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf
void main() {
    uint tid = gl_LocalInvocationID.x;
    ivec2 bufferSize = imageSize(uInHDRBuffer);
    int maxSize = bufferSize.x * bufferSize.y;

    // 1. Step
    // We have half as many threads, as pixels on screen.
    // Each thread grabs two values from the HDR buffer.

    uint i = gl_WorkGroupID.x * gl_WorkGroupSize.x * 2 + tid;
    float left = i < maxSize ? sampleHDRBuffer(indexToPos(i, bufferSize.x)) : 0;

    uint j = i + gl_WorkGroupSize.x;
    float right = j < maxSize ? sampleHDRBuffer(indexToPos(j, bufferSize.x)) : 0;

    // The two values are being combined and written to this threads shared memory address.
    sSum[tid] = left + right;
    sMax[tid] = max(left, right);

    // Wait for all threads in the work group to finish.
    memoryBarrierShared();
    barrier();

    // 2. Step
    // Each thread combines its own value with a value of 2 times its current position.
    // We will halve the amount of threads each turn.
    // We repeat this step until one warp is left.
    // We could do this in a loop, but manual unrolling is faster (I profiled this!).
    if (tid < 256) {
        sSum[tid] += sSum[tid + 256];
        sMax[tid] = max(sMax[tid], sMax[tid + 256]);
    }

    memoryBarrierShared();
    barrier();

    if (tid < 128) {
        sSum[tid] += sSum[tid + 128];
        sMax[tid] = max(sMax[tid], sMax[tid + 128]);
    }

    memoryBarrierShared();
    barrier();

    if (tid < 64) {
        sSum[tid] += sSum[tid + 64];
        sMax[tid] = max(sMax[tid], sMax[tid + 64]);
    }

    memoryBarrierShared();
    barrier();

    // 3. Step
    // We don't need to check the thread id for the last warp, since they are more efficient
    // doing the same work.
    if (tid < 32) {
        sSum[tid] += sSum[tid + 32];
        sMax[tid] = max(sMax[tid], sMax[tid + 32]);
        memoryBarrierShared();
        barrier();

        sSum[tid] += sSum[tid + 16];
        sMax[tid] = max(sMax[tid], sMax[tid + 16]);
        memoryBarrierShared();
        barrier();

        sSum[tid] += sSum[tid + 8];
        sMax[tid] = max(sMax[tid], sMax[tid + 8]);
        memoryBarrierShared();
        barrier();

        sSum[tid] += sSum[tid + 4];
        sMax[tid] = max(sMax[tid], sMax[tid + 4]);
        memoryBarrierShared();
        barrier();

        sSum[tid] += sSum[tid + 2];
        sMax[tid] = max(sMax[tid], sMax[tid + 2]);
        memoryBarrierShared();
        barrier();

        float sum = sSum[tid] + sSum[tid + 1];
        float max = max(sMax[tid], sMax[tid + 1]);

        // The first thread in each work group writes the final value to the output.
        if (tid == 0) {
            imageStore(uOutLuminance, int(gl_WorkGroupID.x), vec4(sum, max, 0.0, 0.0));
        }
    }
}
